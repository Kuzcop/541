{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "from tensorflow.keras import layers, models\n",
    "import time\n",
    "import ast\n",
    "import tensorflow as tf\n",
    "from predictor import Predictor\n",
    "from copy import deepcopy\n",
    "import random\n",
    "from numpy import exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# GLOBAL VARIABLES\n",
    "show = True\n",
    "predictor_data_filename = \"data.txt\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "380ced4168f35f42"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# PREDICTOR MODEL\n",
    "class Predictor:\n",
    "    def __init__(self, n_layers, keras_loss_fun, hyperparameters, show_summary=False):\n",
    "        # Hyperparameters:\n",
    "        # List of dictionaries (one for each layer)\n",
    "        model = models.Sequential()\n",
    "        # Create input and hidden layers\n",
    "        for i in range(n_layers):\n",
    "            model.add(layers.Dense(hyperparameters[i]['size'], activation=hyperparameters[i]['activation']))\n",
    "\n",
    "        # Create output layer (accuracy, test_latency)\n",
    "        model.add(layers.Dense(2))  # No activation function, since it is a regression problem\n",
    "\n",
    "        model.compile(optimizer='adam',\n",
    "                      loss=keras_loss_fun,\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "        if show_summary:\n",
    "            model.summary()\n",
    "\n",
    "        self.model = model\n",
    "\n",
    "    def train(self, train_data, train_labels, test_data, test_labels):\n",
    "\n",
    "        history = self.model.fit(train_data, train_labels, epochs=10, batch_size=32,\n",
    "                                 validation_data=(test_data, test_labels))\n",
    "\n",
    "        start_time = time.time()\n",
    "        test_loss, test_acc = self.model.evaluate(test_data, test_labels)\n",
    "        end_time = time.time()\n",
    "        test_time = end_time - start_time\n",
    "        return test_acc, test_time"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6ce9ae1d458f8571"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# PREDICTOR HELPER\n",
    "\n",
    "# TRAIN AND TEST DATA\n",
    "def _get_predictor_data(filename):\n",
    "    hyperparameters, outputs = _parse_predictor_data_file(filename)\n",
    "    hyperparameters, outputs = _preprocess_predictor_data(hyperparameters, outputs)\n",
    "\n",
    "    dataset_size = len(hyperparameters)\n",
    "    train_percentage = 0.7\n",
    "    train_size = round(dataset_size * train_percentage)\n",
    "\n",
    "    dataset_indexes = range(0, dataset_size)\n",
    "    train_sample_indexes = random.sample(dataset_indexes, train_size)\n",
    "    test_sample_indexes = [index for index in dataset_indexes if index not in train_sample_indexes]\n",
    "\n",
    "    train_data, train_labels = [(hyperparameters[i], outputs[i]) for i in train_sample_indexes]\n",
    "    test_data, test_labels = [(hyperparameters[i], outputs[i]) for i in test_sample_indexes]\n",
    "\n",
    "    return train_data, train_labels, test_data, test_labels\n",
    "\n",
    "\n",
    "activation_mappings = {\n",
    "    'linear': 0,\n",
    "    'relu': 1,\n",
    "    'sigmoid': 5,\n",
    "    'tanh': 6,\n",
    "    'softmax': 8,\n",
    "    'exponential': 10,\n",
    "}\n",
    "\n",
    "padding_mappings = {\n",
    "    'valid': 0,\n",
    "    'same': 1,\n",
    "}\n",
    "\n",
    "\n",
    "def _preprocess_predictor_data(inputs, outputs):\n",
    "    processed_inputs = []\n",
    "    for model in inputs:\n",
    "        processed_model = []\n",
    "        for (layer_type, params) in model:  # Iterate over all layers\n",
    "            if 'conv' in layer_type:\n",
    "                processed_model.append(activation_mappings[params['activation']])\n",
    "                processed_model.append(padding_mappings[params['padding']])\n",
    "                processed_model.append(params['kernel_size'])\n",
    "                processed_model.append(params['strides'])\n",
    "                processed_model.append(params['filters'] / 10)  #  Make it around the same size as the others\n",
    "                processed_model.append(params['pool_size'])\n",
    "            else:\n",
    "                processed_model.append(padding_mappings[params['padding']])\n",
    "                processed_model.append(params['strides'])\n",
    "                processed_model.append(params['pool_size'])\n",
    "        if show:\n",
    "            print(\"\\nModel = {}\".format(model))\n",
    "            print(\"\\n--> Processed model = {}\\n\".format(processed_model))\n",
    "        processed_inputs.append(processed_model)\n",
    "    return processed_inputs, outputs\n",
    "\n",
    "\n",
    "def _parse_predictor_data_file(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    hyperparameters = []\n",
    "    outputs = []  # Output  = (accuracy, latency)\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        result = ast.literal_eval(line)\n",
    "        hyperparameters.append(result['HP'])\n",
    "        outputs.append((result['Accuracy'], result['Latency']))\n",
    "\n",
    "    return hyperparameters, outputs\n",
    "\n",
    "predictor_loss_set = [\n",
    "    'mean_squared_error',\n",
    "    'mean_absolute_error',\n",
    "    tf.keras.losses.Huber(),\n",
    "    tf.keras.losses.LogCosh()\n",
    "]\n",
    "\n",
    "predictor_n_layer_set = range(1, 10)\n",
    "\n",
    "predictor_layer_hp_set = {\n",
    "    'activation': [\n",
    "        'relu',\n",
    "        'selu',\n",
    "        'tanh',\n",
    "        'linear',\n",
    "        'swish',\n",
    "    ],\n",
    "    'size': [20, 40, 60, 80, 100]\n",
    "}\n",
    "\n",
    "\n",
    "def get_predictor_layer_default_hyperparameters():\n",
    "    return {\n",
    "        'activation': random.sample(predictor_layer_hp_set['activation'], 1)[0],\n",
    "        'size': random.sample(predictor_layer_hp_set['size'], 1)[0]\n",
    "    }\n",
    "\n",
    "\n",
    "def get_default_predictor(n_layers, loss):\n",
    "    return {\n",
    "        \"keras_loss_fun\": loss,\n",
    "        \"n_layers\": n_layers,\n",
    "        \"hyperparameters\": [get_predictor_layer_default_hyperparameters() for _ in range(0, n_layers)]\n",
    "    }\n",
    "\n",
    "# GET PREDICTOR DATA\n",
    "training_data, training_labels, testing_data, testing_labels = _get_predictor_data(predictor_data_filename)\n",
    "\n",
    "def predictor_objective(params):\n",
    "    n_layers = params[\"n_layers\"]\n",
    "    keras_loss_fun = params[\"keras_loss_fun\"]\n",
    "    hyperparameters = params[\"hyperparameters\"]\n",
    "    predictor = Predictor(n_layers, keras_loss_fun, hyperparameters)\n",
    "\n",
    "    test_acc, test_time = predictor.train(training_data, training_labels, testing_data, testing_labels)\n",
    "\n",
    "    obj_value = test_acc**2 / test_time\n",
    "    if show:\n",
    "        print(\"\\n\", \"-\" * 8, \"test_accuracy: {} #### test_latency: {} #### objective: {}\"\n",
    "                    .format(test_acc,test_time, obj_value), \"-\" * 8)\n",
    "\n",
    "    return obj_value\n",
    "\n",
    "\n",
    "def predictor_get_random_neighbouring_solution(old_solution):\n",
    "    solution = deepcopy(old_solution)\n",
    "    n_layers = solution[\"n_layers\"]\n",
    "\n",
    "    # Change the size of two layers\n",
    "    change_size_layers = random.sample(range(0, n_layers), 2)\n",
    "    for change_size_layer in change_size_layers:\n",
    "        new_size = random.sample(predictor_layer_hp_set[\"size\"], 1)[0]\n",
    "        solution[\"hyperparameters\"][change_size_layer][\"size\"] = new_size\n",
    "    # Change activation of one layer\n",
    "    change_activation_layer = random.sample(range(0, n_layers), 1)[0]\n",
    "    new_activation = random.sample(predictor_layer_hp_set[\"activation\"], 1)[0]\n",
    "    solution[\"hyperparameters\"][change_activation_layer][\"activation\"] = new_activation\n",
    "\n",
    "    return solution\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9d503fe4b36ad81b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# METAHEURISTICS: SIMULATED ANNEALING\n",
    "def simulated_annealing(initial_solution, neighbour_gen_fun, objective_fun, iterations, c, seed=-1):\n",
    "    # Initialization phase\n",
    "    if seed >= 0:\n",
    "        random.seed = seed\n",
    "    current_solution = initial_solution\n",
    "    current_eval = objective_fun(current_solution, show)\n",
    "    # Optimization phase\n",
    "    for i in range(iterations):\n",
    "        next_solution = neighbour_gen_fun(current_solution)\n",
    "        next_eval = objective_fun(next_solution)\n",
    "        if next_eval > current_eval or current_eval == 0 or _accept_worse_solution(c, i+1, current_eval, next_eval):\n",
    "            current_solution = next_solution\n",
    "            current_eval = next_eval\n",
    "            if show:\n",
    "                print(\"\\n\", \"#\", \"Accepted new solution\")\n",
    "    return current_solution, current_eval\n",
    "\n",
    "\n",
    "def _accept_worse_solution(c, interval, curr_val, next_val):\n",
    "    return random.random() < exp(interval * (next_val - curr_val) / (c * curr_val))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "70b4b5e10c2fcd5f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# OPTIMIZER\n",
    "\n",
    "def _predictor_sa_optimization(n_layers, loss_fun):\n",
    "    c = 1\n",
    "    iterations_per_conf = 50\n",
    "    seed = -1\n",
    "    return simulated_annealing(get_default_predictor(n_layers, loss_fun),\n",
    "                               predictor_get_random_neighbouring_solution,\n",
    "                               predictor_objective,\n",
    "                               iterations_per_conf,\n",
    "                               c,\n",
    "                               seed)\n",
    "\n",
    "def optimize_predictor(optimize_fun):\n",
    "    best_results = {}\n",
    "\n",
    "    for n_layers in predictor_n_layer_set:\n",
    "        for loss_fun in predictor_loss_set:\n",
    "            model, eval = optimize_fun(n_layers, loss_fun)\n",
    "            print(\"#### LAYERS: {}, LOSS: {}: MODEL: {}, EVAL: {} ####\\n\".format(n_layers, loss_fun, model, eval))\n",
    "            best_results[n_layers, loss_fun] = {\n",
    "                \"model\": model,\n",
    "                \"eval\": eval,\n",
    "            }\n",
    "            "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "92829a489153f3c1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# RUN\n",
    "optimize_predictor(_predictor_sa_optimization)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6dd826b54124e685"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
